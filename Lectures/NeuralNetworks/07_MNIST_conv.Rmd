---
jupyter:
  jupytext:
    cell_metadata_json: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %load_ext autoreload
# %autoreload 2
```

```{python slideshow={'slide_type': 'skip'}}
import torch
import torchvision
from torch import nn
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline
plt.rcParams["figure.figsize"] = [12,8]
plt.rcParams["animation.html"] = "jshtml"
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
from IPython.display import Markdown as md
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
import sys
sys.path.append('../../')
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
from mchlearn import tensor_plot
from matplotlib.animation import FuncAnimation
```

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
# Convolutional neural networks
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "skip"}, "tags": []} -->
Here we revisit  the "Hello World!" of machine learning: ["The MNIST Database of hanwritten digits"](http://yann.lecun.com/exdb/mnist/). This seem appropriate as this is the set for which the convolutional networks were actually proposed in [Backpropagation Applied to Handwritten Zip Code Recognition ](https://direct.mit.edu/neco/article/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code).
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
## MNIST
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
This dataset bundled in many machine learning libraries and PyTorch is no exception. 
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
train_data = torchvision.datasets.MNIST('./data/mnist', train=True, download=True)
test_data  = torchvision.datasets.MNIST('./data/mnist', train=False, download=True)
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
train_features   = train_data.data.to(dtype=torch.float32)
train_labels = train_data.targets
```

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
The data consists of 28 by 28 pixels 8bit grayscale images of handwritten digits, the labels are integers denoting corresponding digits:
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
train_data.data.shape
```

```{python slideshow={'slide_type': 'slide'}}
fig_mnist, axes = plt.subplots(2,4, figsize=(16,8))
for i in range(8):
    ax=axes.ravel()[i]
    ax.imshow(train_features[i].numpy(), cmap='Greys');
    ax.set_title(train_labels[i].item(), fontsize=24)
```

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
For the purpose of this notebook I will use only a subset of data. This will make the training of the network much quicker. 
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
n_samples = 12000
```

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
I will also rescale the grayscale values to lie in the interval $[0,1)$. The 2D convolutional layers expect the data as the 4-dimensional tensor $(N,C,H,W)$ where $N$ stands for batch dimension, $C$ for channels adn $H$ and $W$ for height and width respectively so I will reshape the data accordingly: 
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}}
dataset = torch.utils.data.TensorDataset( 
    (train_features[:n_samples]/256.0).view(-1,1,28,28),
    train_labels[:n_samples])
```

<!-- #region {"slideshow": {"slide_type": "skip"}, "tags": []} -->
I will split the  training data into actuall training set and the validation set
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}}
train_dataset, validation_dataset = torch.utils.data.random_split(dataset, (10000,2000))
```

```{python slideshow={'slide_type': 'skip'}}
train_loader = torch.utils.data.DataLoader(train_dataset, 
                                           batch_size = 100, 
                                           shuffle = True)
validation_loader = torch.utils.data.DataLoader(validation_dataset, 
                                           batch_size = 100, 
                                           shuffle = True)
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
test_features   = test_data.data.to(dtype=torch.float32)
test_labels = test_data.targets
test_dataset = torch.utils.data.TensorDataset(
    (test_features/256.0).view(-1,1,28,28), test_labels)
```

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
## The model 
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
Below is a simple convolutional neural network with arcitecture taken from Fran√ßois Chollet's [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) book. I will explain the meaning of each layer in the rest of the notebook. 
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}}
model = torch.nn.Sequential( 
    #28x28
    nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3, stride=1, padding=0), #26x26
    nn.ReLU(),
    nn.MaxPool2d(2,2),#13x13
    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=0), #11x11
    nn.ReLU(),
    nn.MaxPool2d(2,2), #5x5 
    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),#3x3
    nn.Flatten(),
    nn.Linear(in_features=9*64, out_features=64),
    nn.ReLU(),
    nn.Linear(in_features=64, out_features=10)
)
```

```{python slideshow={'slide_type': 'fragment'}}
with torch.no_grad():
    pred = model(train_dataset[:][0])
```

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
Tensor `pred` contains the predicted probabilities for each  digit for each input:
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
pred[:4]
```

<!-- #region {"slideshow": {"slide_type": "skip"}} -->
The accuracy of  clasification can be calculated as follows: 
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}}
def accuracy(pred, labels):
    return torch.sum(torch.argmax(pred,axis = 1)==labels).to(dtype=torch.float32).item()/len(labels)

def model_accuracy(model, dataset):
    features, labels = dataset[:]
    with torch.no_grad():
        pred = model(features)
    return accuracy(pred, labels)
```

```{python slideshow={'slide_type': 'fragment'}}
accuracy(pred, train_dataset[:][1])
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
optim = torch.optim.Adam(model.parameters(), lr=0.001)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
# %%time
ce_loss = torch.nn.CrossEntropyLoss()
for e in range(5):
    for features, labels in train_loader:        
        optim.zero_grad()
        pred = model(features)
        loss = ce_loss(pred,labels)
        
        loss.backward()
        optim.step()   
    with torch.no_grad():
            print(f"{e:3d} {loss.item():.4f} {model_accuracy(model, train_dataset):.4f} {model_accuracy(model, validation_dataset):.4f}")    
 
```

```{python slideshow={'slide_type': 'fragment'}}
with torch.no_grad():
    print(model_accuracy(model, test_dataset))
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Convolutional layer
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
```python
nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3, stride=1, padding=0)
```
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
####  Sparsity (locality)
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
#### Weight sharing
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
### Cross correlation
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$\begin{array}{cccccc}
O_{i,j} = \displaystyle \sum_{k,l=0}^{K-1} & w_{k,l}&\times &I_{i+k,j+l}&+&b_{c}&,\qquad i = 0,\ldots,H_i, \;j = 0,\ldots,W_i \\
& \uparrow & &&&\uparrow&\\
&\text{kernel}& &&&\text{bias}&
\end{array}
$$
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
fig, ax = plt.subplots(figsize=(16,8))
plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)
tensor_plot.draw_grid_with_padding(ax, 5,5, padding=0, left=2, bottom=2)
tensor_plot.draw_kernel(ax,3,0,0, left=2, bottom=2)
tensor_plot.draw_grid_with_padding(ax, 3,3, padding=0, left=13, bottom=3)
tensor_plot.draw_kernel(ax, 1,0,0,  left=13, bottom=3)
ax.arrow(3.5,3.5,10,0,head_width=0.25, head_starts_at_zero=False, length_includes_head=True, shape='full', overhang=1)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
fig
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
from itertools import product
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
kernel_left = None
kernel_right = None
fig, ax = plt.subplots(figsize=(16,8))
#plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)


def init():
  
    tensor_plot.draw_grid_with_padding(ax, 5,5, padding=0, left=2, bottom=2)
    kernel_left = tensor_plot.draw_kernel(ax,3,0,0, left=2, bottom=2)
    kernel_left.set_gid('left')
    tensor_plot.draw_grid_with_padding(ax, 3,3, padding=0, left=13, bottom=3)
    kernel_right = tensor_plot.draw_kernel(ax, 1,0,0,  left=13, bottom=3)
    kernel_right.set_gid('right')
    return ax

    
def update(frame, *fargs):
    
    dy, dx = frame
    
    if fargs[0]['first']:
        fargs[0]['first']=False
        for p in ax.patches:
            if p.get_gid() =='left':
                fargs[0]['left'] = p
                fargs[0]['left_x'] = p.get_x()
                fargs[0]['left_y'] = p.get_y()
            elif p.get_gid()=='right':
                x = p.get_x()
                fargs[0]['right'] = p
                fargs[0]['right_x'] = p.get_x()
                fargs[0]['right_y'] = p.get_y()
                
                
                

    fargs[0]['left'].set_x(fargs[0]['left_x']+dx)
    fargs[0]['left'].set_y(fargs[0]['left_y']+dy)
    
    fargs[0]['right'].set_x(fargs[0]['right_x']+dx)
    fargs[0]['right'].set_y(fargs[0]['right_y']+dy)
    
    
    return ax
    
anim = FuncAnimation(fig, update, frames=product(range(3), range(3)), init_func=init, blit=False, repeat=False, fargs=({'first':True},), interval=1000)
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
anim 
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
### Padding
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
kernel_left = None
kernel_right = None
fig, ax = plt.subplots(figsize=(16,8))
#plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)
ax.set_title('Same padding', fontdict={'fontsize':20})

def init():
  
    tensor_plot.draw_grid_with_padding(ax, 5,5, padding=1, left=2, bottom=2)
    kernel_left = tensor_plot.draw_kernel(ax,3,-1,-1, left=2, bottom=2)
    kernel_left.set_gid('left')
    tensor_plot.draw_grid_with_padding(ax, 5,5, padding=0, left=12, bottom=2)
    kernel_right = tensor_plot.draw_kernel(ax, 1,0,0,  left=12, bottom=2)
    kernel_right.set_gid('right')
    return ax

    
def update(frame, *fargs):
    
    dy, dx = frame
    
    if fargs[0]['first']:
        fargs[0]['first']=False
        for p in ax.patches:
            if p.get_gid() =='left':
                fargs[0]['left'] = p
                fargs[0]['left_x'] = p.get_x()
                fargs[0]['left_y'] = p.get_y()
            elif p.get_gid()=='right':
                x = p.get_x()
                fargs[0]['right'] = p
                fargs[0]['right_x'] = p.get_x()
                fargs[0]['right_y'] = p.get_y()
                
                
                

    fargs[0]['left'].set_x(fargs[0]['left_x']+dx)
    fargs[0]['left'].set_y(fargs[0]['left_y']+dy)
    
    fargs[0]['right'].set_x(fargs[0]['right_x']+dx)
    fargs[0]['right'].set_y(fargs[0]['right_y']+dy)
    
    
    return ax
    
anim = FuncAnimation(fig, update, frames=product(range(0,5), range(0,5)), init_func=init, blit=False, repeat=False, fargs=({'first':True},), interval=1000)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
anim
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
t = torch.arange(9).to(dtype=torch.float).reshape(1,1,3,3)
print(t)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.pad(t,(2,2,2,2),mode = 'constant', value=0) )
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
print(torch.nn.functional.pad(t,(2,2,2,2),mode = 'reflect') )
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
print(torch.nn.functional.pad(t,(2,2,2,2),mode = 'replicate') )
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
print(torch.nn.functional.pad(t,(2,2,2,2),mode = 'circular') )
```

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
kernel_left = None
kernel_right = None
fig, ax = plt.subplots(figsize=(16,8))
#plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)
ax.set_title('Full padding', fontdict={'fontsize':20})

def init():
  
    tensor_plot.draw_grid_with_padding(ax, 5,5, padding=2, left=2, bottom=2)
    kernel_left = tensor_plot.draw_kernel(ax,3,-2,-2, left=2, bottom=2)
    kernel_left.set_gid('left')
    tensor_plot.draw_grid_with_padding(ax, 6,6, padding=0, left=12, bottom=2)
    kernel_right = tensor_plot.draw_kernel(ax, 1,0,0,  left=12, bottom=2)
    kernel_right.set_gid('right')
    return ax

    
def update(frame, *fargs):
    
    dy, dx = frame
    
    if fargs[0]['first']:
        fargs[0]['first']=False
        for p in ax.patches:
            if p.get_gid() =='left':
                fargs[0]['left'] = p
                fargs[0]['left_x'] = p.get_x()
                fargs[0]['left_y'] = p.get_y()
            elif p.get_gid()=='right':
                x = p.get_x()
                fargs[0]['right'] = p
                fargs[0]['right_x'] = p.get_x()
                fargs[0]['right_y'] = p.get_y()
                
                
                

    fargs[0]['left'].set_x(fargs[0]['left_x']+dx)
    fargs[0]['left'].set_y(fargs[0]['left_y']+dy)
    
    fargs[0]['right'].set_x(fargs[0]['right_x']+dx)
    fargs[0]['right'].set_y(fargs[0]['right_y']+dy)
    
    
    return ax
    
anim = FuncAnimation(fig, update, frames=product(range(0,6), range(0,6)), init_func=init, blit=False, repeat=False, fargs=({'first':True},), interval=1000)
```

```{python slideshow={'slide_type': 'slide'}, tags=c()}
anim 
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Stride
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
kernel_left = None
kernel_right = None
fig, ax = plt.subplots(figsize=(16,8))
#plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)
# ax.set_title('Full padding', fontdict={'fontsize':20})

def init():
  
    tensor_plot.draw_grid_with_padding(ax, 5,5, padding=1, left=2, bottom=2)
    kernel_left = tensor_plot.draw_kernel(ax,3,-1,-1, left=2, bottom=2)
    kernel_left.set_gid('left')
    tensor_plot.draw_grid_with_padding(ax, 3,3, padding=0, left=14, bottom=4)
    kernel_right = tensor_plot.draw_kernel(ax, 1,0,0,  left=14, bottom=4)
    kernel_right.set_gid('right')
    return ax

    
def update(frame, *fargs):
    
    dy, dx = frame
    
    if fargs[0]['first']:
        fargs[0]['first']=False
        for p in ax.patches:
            if p.get_gid() =='left':
                fargs[0]['left'] = p
                fargs[0]['left_x'] = p.get_x()
                fargs[0]['left_y'] = p.get_y()
            elif p.get_gid()=='right':
                x = p.get_x()
                fargs[0]['right'] = p
                fargs[0]['right_x'] = p.get_x()
                fargs[0]['right_y'] = p.get_y()
                
                
                

    fargs[0]['left'].set_x(fargs[0]['left_x']+dx)
    fargs[0]['left'].set_y(fargs[0]['left_y']+dy)
    
    fargs[0]['right'].set_x(fargs[0]['right_x']+dx/2)
    fargs[0]['right'].set_y(fargs[0]['right_y']+dy/2)
    
    
    return ax
    
anim = FuncAnimation(fig, update, frames=product(range(0,6,2), range(0,6,2)), init_func=init, blit=False, repeat=False, fargs=({'first':True},), interval=1000)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
anim 
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Input channels
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$O_{i,j} =\displaystyle \sum_{k,l=0}^{K-1} w_{c',k,l}\times I_{c',-P_i+i+k, -P_i+j+l}+b,\qquad i = 0,\ldots,H_i-2 P_i, \;j = 0,\ldots,W_i-2 P_i$$
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Output chanels -- Filters
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$(N_i,C_i,H_i,W_i)\longrightarrow (N_i,C_o,H_o,W_o)$$
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$O_{c,i,j} = \sum_{c'=0}^{C_i-1} \displaystyle \sum_{k,l=0}^{K-1} w_{c,c'k,l}I_{c',-P_i+i+k, -P_i+j+l}+b_{c},\qquad c=0,\ldots C_o-1,\;i = 0,\ldots,H_i-2 P_i, \;j = 0,\ldots,W_i-2 P_i$$
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
# Pooling layers
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
```python
nn.MaxPool2d(2,2),#13x13
```
<!-- #endregion -->

```{python slideshow={'slide_type': 'skip'}, tags=c()}
# %%capture
fig, ax = plt.subplots(figsize=(16,8))
plt.axis("off")
ax.set_aspect(1.0)
ax.set_ylim(-0.5,  9.5)
ax.set_xlim(-0.5, 19.5)
tensor_plot.draw_grid_with_padding(ax, 5,5, padding=0, left=2, bottom=2)
tensor_plot.draw_kernel(ax,2,0,0, left=2, bottom=2)
tensor_plot.draw_grid_with_padding(ax, 2,2, padding=0, left=13, bottom=3)
tensor_plot.draw_kernel(ax, 1,0,0,  left=13, bottom=3)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
fig
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
### Max pooling
<!-- #endregion -->

$$o = \max_{i,j} x_{i,j}$$

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
t = torch.randint(0,5,(5,5)).to(dtype=torch.float).reshape(1,1,5,5)
print(t)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.max_pool2d(t, kernel_size=2))
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.max_pool2d(t, kernel_size=2, stride=1))
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
### Average pooling
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$o = \frac{1}{K_w\times K_h}\sum_{i,j} x_{i,j} $$
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(t)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.avg_pool2d(t, kernel_size=2))
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Power average pooling
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}, "tags": []} -->
$$o = \sqrt[\leftroot{20}\uproot{2}\textstyle p]{\sum_{i,j}x_{i,j}^p}$$
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(t)
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.lp_pool2d(t, kernel_size=2, norm_type=1))
```

```{python slideshow={'slide_type': 'fragment'}, tags=c()}
print(torch.nn.functional.lp_pool2d(t, kernel_size=2, norm_type=50))
```

<!-- #region {"slideshow": {"slide_type": "slide"}, "tags": []} -->
## Classifier
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "fragment"}} -->
```python
nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),#3x3
nn.Flatten(),
nn.Linear(in_features=9*64, out_features=64),
nn.ReLU(),
nn.Linear(in_features=64, out_features=10)
```
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
```python
model = torch.nn.Sequential( #1x28x28
nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3, stride=1, padding=0), #32x26x26
nn.ReLU(),
nn.MaxPool2d(2,2),#32x13x13
nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=0), #64,11x11
nn.ReLU(),
nn.MaxPool2d(2,2), #64x5x5 
nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),#64x3x3
nn.Flatten(),#576
nn.Linear(in_features=576, out_features=64)#64
nn.ReLU(),
nn.Linear(in_features=64, out_features=10)#10
)
```
<!-- #endregion -->

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
## VGG16
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}}
from IPython.display import Image
Image(filename='vgg16.png')
```

```{python slideshow={'slide_type': 'slide'}}
import torchvision.models as models

vgg16 = models.vgg16()
```

```{python slideshow={'slide_type': 'fragment'}}
n_param = 0
for p in vgg16.parameters():
    print(p.shape, p.nelement())
    n_param+= p.nelement()

```

```{python slideshow={'slide_type': 'fragment'}}
print(f"{n_param/(2**20):.2f}M parameters")    
```

```{python slideshow={'slide_type': 'fragment'}}
print(f"{4*n_param/(2**20):.2f}MB")    
```

```{python slideshow={'slide_type': 'slide'}}
import inspect
for name, obj in  inspect.getmembers(models):
    if inspect.isclass(obj):
        print(name)
        
```

```{python slideshow={'slide_type': 'slide'}}
import inspect
for name, obj in  inspect.getmembers(models):
    if inspect.isfunction(obj):
        print(name)
        
```

<!-- #region {"slideshow": {"slide_type": "slide"}} -->
## Transfer learning
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}}
vgg16 = models.vgg16(pretrained=True)
```
